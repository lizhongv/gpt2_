{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/main/en/model_doc/gpt2\n",
    "\n",
    "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "*GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text.*\n",
    "\n",
    "*a causal language modeling (CLM) objective.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 一、GPT2Model\n",
    "\n",
    "> *The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T18:35:26.421157900Z",
     "start_time": "2024-03-26T18:35:26.100399600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "model_id = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "# ? tokenizer.eos_token  is ''\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# or tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# not  tokenizer.pad_token='[PAD]'\n",
    "# https://github.com/huggingface/transformers/issues/22312#issuecomment-1479544526\n",
    "# https://github.com/huggingface/transformers/issues/22312#issuecomment-1479574070\n",
    "# https://github.com/huggingface/transformers/issues/22312#issuecomment-1482588993\n",
    "\n",
    "# Indeed the original sentencepiece model does not have a padding token.\n",
    "# You can probably pad using the eos_token like it is done for GPT2.\n",
    "\n",
    "model = GPT2Model.from_pretrained(model_id)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids[0]))\n",
    "\n",
    "# forward\n",
    "outputs = model(\n",
    "    **inputs,   # input_ids, attention_mask\n",
    "\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    return_dict=True,\n",
    "\n",
    "    use_cache=True,  # decoder-only & inference\n",
    ")\n",
    "\n",
    "outputs.keys()\n",
    "# outputs.hidden_states  # 13,  [1,6,768]\n",
    "# outputs.last_hidden_state # [1,6,768]\n",
    "# outputs.past_key_values # 12, [1,12,6,64]\n",
    "# outputs.attentions  # 12, [1,12,6,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first next token\n",
    "last_hidden_state # [1, 6, 768]\n",
    "hidden_states # 13, [1, 6, 768]  12+last_hidden_state\n",
    "attentions(probs) # 12, [1, 12, 6, 6]\n",
    "past_key_values # 12, 2, [1, 12, 6, 64] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、 GPT2LMHeadModel\n",
    "\n",
    "> *The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).*\n",
    "\n",
    "https://huggingface.co/blog/zh/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# model_id = \"/data0/lizhong/models/GPT/gpt2\"\n",
    "# device = \"cuda:0\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "# model.eval()\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "device = \"cuda:0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    \"Hello, my dog is cute\", return_tensors='pt').to(model.device)\n",
    "\n",
    "# self.lm_head.weight is self.transformer.wte.weight  # True\n",
    "\n",
    "# forward\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    labels=input_ids,  # 注意：可有，可无\n",
    "\n",
    "    return_dict=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "\n",
    "    # use_cache=True,\n",
    ")\n",
    "\n",
    "outputs.keys()\n",
    "# outputs.attentions\n",
    "# outputs.hidden_states\n",
    "# outputs.past_key_values\n",
    "# outputs.logits\n",
    "# outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores', 'logits', 'attentions', 'hidden_states', 'past_key_values'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# model_id = \"/data0/lizhong/models/GPT/gpt2\"\n",
    "# device = \"cuda:0\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_id = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "device = \"cuda:0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    \"Hello, my dog is cute\", return_tensors='pt').to(model.device)\n",
    "\n",
    "\n",
    "# generate\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=10,\n",
    "\n",
    "    return_dict_in_generate=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "\n",
    "    output_scores=True,\n",
    "    output_logits=True,\n",
    ")\n",
    "\n",
    "\n",
    "outputs.keys()\n",
    "# outputs.attentions\n",
    "# outputs.hidden_states\n",
    "# outputs.past_key_values\n",
    "# outputs.logits\n",
    "# outputs.loss\n",
    "# outputs.sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、不同的解码策略\n",
    "\n",
    "http://fancyerii.github.io/2023/12/19/hg-transformer-generate/\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopping_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.stopping_criteria import StoppingCriteria, StoppingCriteriaList, \\\n",
    "    STOPPING_CRITERIA_INPUTS_DOCSTRING, add_start_docstrings\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "class StopAtSpecificTokenCriteria(StoppingCriteria):\n",
    "    \"\"\"\n",
    "    当生成出第一个指定token时，立即停止生成\n",
    "    ---------------\n",
    "    ver: 2023-08-02\n",
    "    by: changhongyu\n",
    "    \"\"\"\n",
    "    def __init__(self, token_id_list: List[int] = None):\n",
    "        \"\"\"\n",
    "        :param token_id_list: 停止生成的指定token的id的列表\n",
    "        \"\"\"\n",
    "        self.token_id_list = token_id_list\n",
    "        \n",
    "    @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # return np.argmax(scores[-1].detach().cpu().numpy()) in self.token_id_list\n",
    "        # 储存scores会额外占用资源，所以直接用input_ids进行判断\n",
    "        return input_ids[0][-1].detach().cpu().numpy() in self.token_id_list\n",
    "\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList()\n",
    "stopping_criteria.append(StopAtSpecificTokenCriteria(token_id_list=[13])) # '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logits_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.custom_logits_processor at 0x7f1365586a70>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "class custom_logits_processor(LogitsProcessor):\n",
    "    def __init__(self, forbid_token_id_list: List[int] = None):\n",
    "        self.forbid_token_id_list = forbid_token_id_list\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        for id_ in self.forbid_token_id_list:\n",
    "            scores[:, id_] = -float('inf')\n",
    "        return scores\n",
    "\n",
    "\n",
    "logits_processor_list = LogitsProcessorList([custom_logits_processor([50256, 38537]),])\n",
    "logits_processor_list\n",
    "# https://discuss.huggingface.co/t/use-custom-logitsprocessor-in-model-generate/11603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256, 38537, 13]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['beauty','danger', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7454, 2402,  257,  640]], device='cuda:1')\n",
      "tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
      "         8737,  290, 1049, 3514,   13]], device='cuda:1')\n",
      "Once upon a time, the world was a place of great beauty and great danger.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# model.config\n",
    "\n",
    "# 模型的解码策略是在模型的生成配置中定义的 generation_config\n",
    "# model.generation_config 只会显示与默认生成配置不同的值，而不列出任何默认值。\n",
    "# http://fancyerii.github.io/2023/12/19/hg-transformer-generate/\n",
    "# https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "# https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(input_ids)\n",
    "\n",
    "# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# generate\n",
    "# https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/text_generation#transformers.GenerationMixin\n",
    "# https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/text_generation#transformers.GenerationMixin.generate\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    logits_processor=logits_processor_list,  # 修改当前step在词表空间上的概率分布\n",
    "    stopping_criteria=stopping_criteria,  # 根据用户所规定的规则来中止生成\n",
    "    # streamer=streamer,\n",
    ")  \n",
    "# 通过直接将参数及其值传递给generate方法来覆盖任何generation_config, 例如 max_length, max_new_tokens, do_sample, top_k, eos_token_id...\n",
    "print(output)\n",
    "# tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
    "#          8737,  290, 1049, 3514,   13,  383,  995,  373]], device='cuda:1')\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "# Once upon a time, the world was a place of great beauty and great danger. The world was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T13:27:23.779862400Z",
     "start_time": "2024-03-26T13:27:23.731340700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.transformer\n",
    "model.transformer.get_input_embeddings().weight\n",
    "\n",
    "# model.lm_head\n",
    "model.get_output_embeddings().weight\n",
    "\n",
    "import torch\n",
    "torch.equal(\n",
    "    model.transformer.get_input_embeddings().weight, \n",
    "    model.get_output_embeddings().weight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两种搜索解码：do_sample=False\n",
    "\n",
    "\n",
    "#### (1) greedy search  (num_beams=1)\n",
    "在每个时间步中，选择预测概率分布中概率最大的作为下一个token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,\n",
      "         8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,\n",
      "         3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049]],\n",
      "       device='cuda:1')\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "# (1) greedy\n",
    "generation_config = GenerationConfig(\n",
    "    # max_lehgth=30,\n",
    "    max_new_tokens=30, \n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)  beam search  (num_beams>1)\n",
    "在每个时间步中，选择预测概率分布的前num_beam作为候选继续进行搜索，在所有候选的完整序列（路径中）选择概率最大的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   340,   373,   531,   326,   262,\n",
      "          4453,   550,   531,   284, 19010,    11,   366,    40,   481,  1577,\n",
      "           345,   262,  8251,   286,   262, 13239,   286,  9538,    11,   290,\n",
      "           314,   481,  1577,   345]], device='cuda:1')\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Once upon a time, it was said that the Lord had said to Moses, \"I will give you the keys of the kingdom of heaven, and I will give you\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "# (2) beam search\n",
    "generation_config = GenerationConfig(\n",
    "    # max_lehgth=30,\n",
    "    max_new_tokens=30,\n",
    "    num_beams=3, \n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两种采样解码  do_sample=True\n",
    "\n",
    "(1) Multinomial sampling（num_beams=1）\n",
    "\n",
    "多项式采样生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T12:00:44.002605700Z",
     "start_time": "2024-03-26T12:00:42.902441200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor([[ 7454,  2402,   257,   640,   339,   531,   284,  2241,    11,   366,\n",
      "            40,   714,  1107,   466,   340,   329,   720,  1270,    13,  2011,\n",
      "         17695,   318,  1642,   257,  1256,   286,  1637,   257,   614,    11,\n",
      "           826,   526,  1406,    11]], device='cuda:1')\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Once upon a time he said to himself, \"I could really do it for $30. My grandfather is making a lot of money a year, right.\" So,\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "#  Multinomial sampling\n",
    "generation_config = GenerationConfig(\n",
    "    # max_lehgth=30,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True, \n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) beam search + multionmial sampling （num_beams>1）\n",
    "\n",
    "多项式采样 + 束搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor([[7454, 2402,  257,  640,   11,  340,  373,  531,  326,  611,  257,  582,\n",
      "          550,  262,  826,  284,  257, 3656,   11,  339,  815,  423,  262,  826,\n",
      "          284,  257, 3367,   11,  290,  262,  826,  284,  257, 4957]],\n",
      "       device='cuda:1')\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Once upon a time, it was said that if a man had the right to a wife, he should have the right to a son, and the right to a daughter\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "#  Multinomial sampling\n",
    "generation_config = GenerationConfig(\n",
    "    # max_lehgth=30,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True, \n",
    "    num_beams=3,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 需要注意几点：\n",
    "（1）beam-based methods\n",
    "\n",
    " `early_stopping`\n",
    "\n",
    "It accepts the following values（ Controls the stopping condition）\n",
    "\n",
    "True, where the generation stops as soon as there are num_beams complete candidates; \n",
    "\n",
    "False, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates; \n",
    "\n",
    "\"never\", where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T11:58:51.777967500Z",
     "start_time": "2024-03-26T11:58:51.208956100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   340,   373,   531,   326,   262,\n",
      "          4453,   550,   531,   284, 19010,    11,   366,    40,   481,  1577,\n",
      "           345,   262,  8251,   286,   262, 13239,   286,  9538,    11,   290,\n",
      "           314,   481,  1577,   345]], device='cuda:1')\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Once upon a time, it was said that the Lord had said to Moses, \"I will give you the keys of the kingdom of heaven, and I will give you\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "# (2) beam search\n",
    "generation_config = GenerationConfig(\n",
    "    # max_lehgth=30,\n",
    "    max_new_tokens=30,\n",
    "    num_beams=3, \n",
    "    early_stopping=True,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）beam-based methods\n",
    "\n",
    "`num_return_sequences`\n",
    " \n",
    " The number of independently computed returned sequences for each element in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   340,   373,   531,   326,   262,\n",
      "          4453,   550,   531,   284, 19010,    11,   366,    40,   481,  1577,\n",
      "           345,   262,  8251,   286,   262, 13239,   286,  9538,    11,   290,\n",
      "           314,   481,  1577,   345],\n",
      "        [ 7454,  2402,   257,   640,    11,   340,   373,   531,   326,   262,\n",
      "          4453,   550,   531,   284, 19010,    11,   366,    40,   481,  1577,\n",
      "           345,   262,  8251,   286,   262, 13239,   286,  9538,    11,   290,\n",
      "           314,   481,   787,   345],\n",
      "        [ 7454,  2402,   257,   640,    11,   340,   373,   531,   326,   262,\n",
      "          4453,   550,   531,   284, 19010,    11,   366,    40,   481,  1577,\n",
      "           345,   262,  8251,   286,   262, 13239,   286,  9538,    11,   290,\n",
      "           345,  2236,   423, 43866]], device='cuda:1')\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Once upon a time, it was said that the Lord had said to Moses, \"I will give '\n",
      " 'you the keys of the kingdom of heaven, and I will give you',\n",
      " 'Once upon a time, it was said that the Lord had said to Moses, \"I will give '\n",
      " 'you the keys of the kingdom of heaven, and I will make you',\n",
      " 'Once upon a time, it was said that the Lord had said to Moses, \"I will give '\n",
      " 'you the keys of the kingdom of heaven, and you shall have dominion']\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "# (2) beam search\n",
    "generation_config = GenerationConfig(\n",
    "    # max_lehgth=30,\n",
    "    max_new_tokens=30,\n",
    "    num_beams=3, \n",
    "    # early_stopping=True,\n",
    "    num_return_sequences=3,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs)\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "from pprint import pprint\n",
    "pprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 四、return_dict_in_generate\n",
    "\n",
    " Whether or not to return a ModelOutput instead of a plain tuple.\n",
    " \n",
    "https://zhuanlan.zhihu.com/p/383585103\n",
    "\n",
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "odict_keys(['sequences', 'sequences_scores', 'scores', 'logits', 'beam_indices', 'attentions', 'hidden_states', 'past_key_values'])\n"
     ]
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# device = \"cuda:1\"\n",
    "# model_name_or_path = \"/data0/lizhong/models/gpt/gpt2\"\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "device = \"cuda:1\"\n",
    "model_name_or_path = \"/data0/lizhong/models/gpts/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs = tokenizer(prompt, return_tensors='pt')  # input_ids, attention_mask\n",
    "print(inputs)\n",
    "for key in inputs:\n",
    "    inputs[key] = inputs[key].to(model.device)\n",
    "\n",
    "# (2) beam search\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=10,\n",
    "    # max_new_tokens=30,\n",
    "    num_beams=3, \n",
    "    # early_stopping=True,\n",
    "    num_return_sequences=3,\n",
    "\n",
    "    return_dict_in_generate=True,\n",
    "    \n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    output_scores=True,\n",
    "    output_logits=True, \n",
    "\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "\n",
    ")\n",
    "# generation_config.save_pretrained(\".myGenerationConfig\", push_to_hub=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    # input_ids,\n",
    "    **inputs,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "print(outputs.keys())\n",
    "# print(\"Output:\\n\" + 100 * '-')\n",
    "# # print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "# from pprint import pprint\n",
    "# pprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_t^{T}\\log p(y_t|x,y_{<t})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['Once upon a time, it was said that the',\n",
      " 'Once upon a time, it was said, the',\n",
      " 'Once upon a time, it was said, \"']\n",
      "\n",
      "Output ids:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[7454, 2402,  257,  640,   11,  340,  373,  531,  326,  262],\n",
      "        [7454, 2402,  257,  640,   11,  340,  373,  531,   11,  262],\n",
      "        [7454, 2402,  257,  640,   11,  340,  373,  531,   11,  366]],\n",
      "       device='cuda:1')\n",
      "\n",
      "Input ids:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[7454, 2402,  257,  640]], device='cuda:1')\n",
      "\n",
      "Generate ids: torch.Size([3, 6])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[ 11, 340, 373, 531, 326, 262],\n",
      "        [ 11, 340, 373, 531,  11, 262],\n",
      "        [ 11, 340, 373, 531,  11, 366]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "pprint(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True))\n",
    "\n",
    "print(\"\\nOutput ids:\\n\" + 100 * '-')\n",
    "pprint(outputs.sequences)\n",
    "\n",
    "print(\"\\nInput ids:\\n\"  + 100 * '-')\n",
    "pprint(inputs.input_ids)\n",
    "\n",
    "print(f\"\\nGenerate ids: {gen_sequences.shape}\\n\"  + 100 * '-')\n",
    "gen_sequences = outputs.sequences[:, inputs.input_ids.shape[-1]:]\n",
    "pprint(gen_sequences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 scores\n",
    "https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T16:47:15.352271500Z",
     "start_time": "2024-03-26T16:47:15.351284Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores: 6 torch.Size([3, 50257])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Probs: torch.Size([3, 6, 50257])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "gen_sequences:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[ 11, 340, 373, 531, 326, 262],\n",
      "        [ 11, 340, 373, 531,  11, 262],\n",
      "        [ 11, 340, 373, 531,  11, 366]], device='cuda:1')\n",
      "\n",
      "gen_probs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor(0.4269, device='cuda:1') tensor(0.0399, device='cuda:1') tensor(4.6807e-05, device='cuda:1') tensor(0.0442, device='cuda:1') tensor(0.0001, device='cuda:1') tensor(0.1809, device='cuda:1')\n",
      "tensor(0.4269, device='cuda:1') tensor(4.9092e-05, device='cuda:1') tensor(0.1408, device='cuda:1') tensor(0.0005, device='cuda:1') tensor(0.0002, device='cuda:1') tensor(0.1608, device='cuda:1')\n",
      "tensor(0.4269, device='cuda:1') tensor(2.1966e-05, device='cuda:1') tensor(0.3262, device='cuda:1') tensor(0.0028, device='cuda:1') tensor(0.3376, device='cuda:1') tensor(0.0036, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    " # generate 6 tokens \n",
    "print(f\"\\nScores: {len(outputs.scores)} {outputs.scores[0].shape}\\n\"  + 100 * '-')\n",
    "# pprint(outputs.scores) \n",
    "\n",
    "# let's stack the logits generated at each step to a tensor and transform logits to probs\n",
    "import torch\n",
    "probs = torch.stack(outputs.scores, dim=1).softmax(-1) \n",
    "print(f\"\\nProbs: {probs.shape}\\n\"  + 100 * '-')\n",
    "# pprint(probs)\n",
    "\n",
    "print(f\"\\ngen_sequences:\\n\"  + 100 * '-')\n",
    "pprint(gen_sequences)\n",
    "\n",
    "\n",
    "print(f\"\\ngen_probs:\\n\"  + 100 * '-')\n",
    "print(probs[0][0][11], probs[0][1][340], probs[0][2][373], probs[0][3][531], probs[0][4][326], probs[0][5][262])\n",
    "print(probs[1][0][11], probs[1][1][340], probs[1][2][373], probs[1][3][531], probs[1][4][11], probs[1][5][262])\n",
    "print(probs[2][0][11], probs[2][1][340], probs[2][2][373], probs[2][3][531], probs[2][4][11], probs[2][5][366])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gen_probs: torch.Size([3, 6])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[4.2690e-01, 3.9930e-02, 4.6807e-05, 4.4169e-02, 1.2273e-04, 1.8086e-01],\n",
      "        [4.2690e-01, 4.9092e-05, 1.4079e-01, 4.9947e-04, 1.5632e-04, 1.6078e-01],\n",
      "        [4.2690e-01, 2.1966e-05, 3.2620e-01, 2.7762e-03, 3.3758e-01, 3.5912e-03]],\n",
      "       device='cuda:1')\n",
      "\n",
      "Unique_prob_per_sequence:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([7.8224e-13, 3.7039e-14, 1.0295e-11], device='cuda:1')\n",
      "\n",
      "Log_prob_per_sequence:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([-27.8766, -30.9268, -25.2993], device='cuda:1')\n",
      "\n",
      "Sequences_scores:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([-1.8291, -1.8709, -1.9963], device='cuda:1')\n",
      "\n",
      "normed_gen_probs:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([[3.3333e-01, 9.9822e-01, 1.0022e-04, 9.3096e-01, 3.6325e-04, 5.2388e-01],\n",
      "        [3.3333e-01, 1.2273e-03, 3.0145e-01, 1.0527e-02, 4.6267e-04, 4.6572e-01],\n",
      "        [3.3333e-01, 5.4915e-04, 6.9845e-01, 5.8514e-02, 9.9917e-01, 1.0402e-02]],\n",
      "       device='cuda:1')\n",
      "\n",
      "unique_normed_prob_per_sequence:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tensor([5.9079e-09, 2.7974e-10, 7.7756e-08], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "# pprint(gen_sequences[:,:,None]) # torch.Size([3, 6, 1]) \n",
    "\n",
    "# now we need to collect the probability of the generated token\n",
    "# we need to add a dummy dim in the end to make gather work\n",
    "print(f\"\\nGen_probs: {gen_probs.shape}\\n\"  + 100 * '-')\n",
    "gen_probs = torch.gather(probs, 2, gen_sequences[:, :, None]).squeeze(-1) \n",
    "pprint(gen_probs)\n",
    "\n",
    "# now we can do all kinds of things with the probs\n",
    "\n",
    "# 1) the probs that exactly those sequences are generated again those are normally going to be very small\n",
    "unique_prob_per_sequence = gen_probs.prod(-1)\n",
    "print(f\"\\nUnique_prob_per_sequence:\\n\"  + 100 * '-')\n",
    "print(unique_prob_per_sequence)\n",
    "\n",
    "print(f\"\\nLog_prob_per_sequence:\\n\"  + 100 * '-')\n",
    "print(torch.log(unique_prob_per_sequence))\n",
    "\n",
    "print(f\"\\nSequences_scores:\\n\"  + 100 * '-')\n",
    "print(outputs.sequences_scores)\n",
    "\n",
    "\n",
    "# 2) normalize the probs over the three sequences\n",
    "print(f\"\\nnormed_gen_probs:\\n\"  + 100 * '-')\n",
    "normed_gen_probs = gen_probs / gen_probs.sum(0)\n",
    "print(normed_gen_probs)\n",
    "\n",
    "# assert normed_gen_probs[:, 0].sum() == 1.0, \"probs should be normalized\"\n",
    "\n",
    "# 3) compare normalized probs to each other like in 1)\n",
    "print(f\"\\nunique_normed_prob_per_sequence:\\n\"  + 100 * '-')\n",
    "unique_normed_prob_per_sequence = normed_gen_probs.prod(-1)\n",
    "print(unique_normed_prob_per_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([3, 12, 9, 64])\n",
      "torch.Size([3, 12, 9, 64])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs.past_key_values))  # 3个seq，12head，9token，64dim\n",
    "print(outputs.past_key_values[0][0].shape) # key\n",
    "print(outputs.past_key_values[0][1].shape) # value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "12\n",
      "torch.Size([3, 12, 4, 4])\n",
      "torch.Size([3, 12, 1, 5])\n",
      "torch.Size([3, 12, 1, 6])\n",
      "torch.Size([3, 12, 1, 7])\n",
      "torch.Size([3, 12, 1, 8])\n",
      "torch.Size([3, 12, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs.attentions)) # 生成6个新token\n",
    "print(len(outputs.attentions[0]))  # 生成每个token时，保存12层attention_probs\n",
    "print(outputs.attentions[0][11].shape)  # 生成第一个token时，需要计算prompt中所有token之间的关系\n",
    "print(outputs.attentions[1][11].shape)  # 其他只需计算新token与之前的token之间的关系即可\n",
    "print(outputs.attentions[2][11].shape)\n",
    "print(outputs.attentions[3][11].shape)\n",
    "print(outputs.attentions[4][11].shape)\n",
    "print(outputs.attentions[5][11].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12, 4, 4])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8461, 0.1539, 0.0000, 0.0000],\n",
      "        [0.6469, 0.2558, 0.0973, 0.0000],\n",
      "        [0.6288, 0.1199, 0.1860, 0.0653]], device='cuda:1')\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8461, 0.1539, 0.0000, 0.0000],\n",
      "        [0.6469, 0.2558, 0.0973, 0.0000],\n",
      "        [0.6288, 0.1199, 0.1860, 0.0653]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(outputs.attentions[0][-1].shape)  # 生成第一个token，最后一层attention_probs\n",
    "# torch.Size([3, 12, 4, 4])  3个seq, 12个头\n",
    "pprint(outputs.attentions[0][-1][0][-1]) # 第1个seq，最后head\n",
    "pprint(outputs.attentions[0][-1][1][-1]) # 第2个seq，最后head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "13\n",
      "torch.Size([3, 4, 768])\n",
      "torch.Size([3, 1, 768])\n",
      "torch.Size([3, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs.hidden_states))\n",
    "print(len(outputs.hidden_states[0]))\n",
    "print(outputs.hidden_states[0][12].shape)\n",
    "print(outputs.hidden_states[1][12].shape)\n",
    "print(outputs.hidden_states[5][12].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  if labels is not None:\n",
    "#     # move labels to correct device to enable model parallelism\n",
    "#     labels = labels.to(lm_logits.device)\n",
    "#     # Shift so that tokens < n predict n\n",
    "#     shift_logits = lm_logits[..., :-1, :].contiguous()  # \n",
    "#     shift_labels = labels[..., 1:].contiguous()  #\n",
    "#     # Flatten the tokens\n",
    "#     loss_fct = CrossEntropyLoss()\n",
    "#     loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu117-flash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
